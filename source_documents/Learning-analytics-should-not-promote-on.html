<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<title>Learning analytics should not promote one size fits all: The effects of instructional conditions in predicting academic success</title>
</head>
<body>
<h1><a href="https://djon.es/blog/2016/04/03/learning-analytics-should-not-promote-one-size-fits-all-the-effects-of-instructional-conditions-in-predicting-academic-success/">Learning analytics should not promote one size fits all: The effects of instructional conditions in predicting academic success</a></h1>

<p>Tags: irac, pirac, Uncategorized</p>

<p>What follows is a summary of
<p>Gašević, D., Dawson, S., Rogers, T., &amp; Gasevic, D. (2015). Learning analytics should not promote one size fits all: The effects of instructional conditions in predicating learning success. <i>The Internet and Higher Education</i>, <i>28</i>, 68–84. doi:doi:10.1016/j.iheduc.2015.10.002</p>
I've skimmed it before, but renewed interest is being driven by a local project to explore what analytics might reveal about 9 teacher education courses, especially in light of the <a href="http://qilt.edu.au/">QILT</a> process and data.
<h3>Reactions</h3>
Good paper.

Connections to the work we're doing in terms of similar number of courses (9) and a focus on looking into the diversity hidden by aggregated and homogenised data analysis. The differences are
<ul>
	<li>we're looking at the question of engagement, not prediction (necessarily);</li>
	<li>we're looking for differences within a single discipline/program and aiming to explore diversity within/across a program</li>
	<li>in particular, what it might reveal about our assumptions and practices</li>
	<li>some of our offering are online only</li>
</ul>
<h3>Summary</h3>
Gašević, et al (2015) looks at the influence of specific instructional conditions in 9 blended courses on success prediction using learning analytics and log-data.
<p style="padding-left: 30px;">A lack of attention to instructional conditions can lead to an over or under estimation of the effects of LMS features on students' academic success</p>

<h3>Learning analytics</h3>
Interest in, but questions around the portability of learning analytics.
<p style="padding-left: 30px;">the paper aims to empirically demonstrate the importance for understanding the course and disciplinary context as an essential step when developing and interpreting predictive models of academic success and attrition (Lockyer, Heathcote, &amp; Dawson, 2013)</p>
<strong>Some aims to decontextualise</strong> - i.e. that some work aims to identify predictive models that can
<p style="padding-left: 30px;">inform a generalized model of predictive risk that acts independently of contextual factors such as institution, discipline, or learning design. These omissions of contextual variables are also occasionally expressed as an overt objective.</p>
While there are some large scale projects, most are small scale and (emphasis added)
<p style="padding-left: 30px;">small sample sizes and disciplinary <strong>homogeneity</strong> adds further complexity in interpreting the research findings, leaving open the possibility that disciplinary context and course specific effects may be contributing factors</p>
 <strong>Absence of theory in learning analytics </strong>- at least until recently.  Theory that points to the influence of diversity in context, subject, teacher, and learner.
<p style="padding-left: 30px;">Most post-behaviorist learning theories would suggest the importance of elements of the specific learning situation and student and teacher intentions</p>
<strong>Impact of context </strong>- Mentions Finnegan, Morris and Lee (2009) as a study that looked at the role of contextual variables and finding disciplinary differences and "no single significant predictor shared across all three disciplines"

<strong>Role of theoretical frameworks </strong>- argument for benefits of integrating theory
<ul>
	<li>connect with prior research;</li>
	<li>make clear the aim of research designs and thus what outcomes mean.</li>
</ul>
<h3>Theoretical grounding for study</h3>
Winne and Hadwin's "constructivist, meta-cognitive approach to self-regulated learning
<ol>
	<li>learners construct their knowledge by using tools (cognitive, physical, and digital);</li>
	<li>to operate on raw information (stuff given by courses);</li>
	<li>to construct products of their learning;</li>
	<li>learning products are evaluated via internal and external standards</li>
	<li>learners make decisions about their the tactics and standards used.</li>
	<li>decisions are influenced by internal and external conditions</li>
</ol>
Leading to the idea proposition
<p style="padding-left: 30px;">that learning analytics must account for conditions in order to make any meaningful interpretation of learning success prediction</p>
The focus here is on <em>instructional conditions</em>.

<strong>Predictions from this</strong>
<ol>
	<li>Students will tend to interact more with recommended tools</li>
	<li>There will be a positive relationship between students level of interaction and the instructional conditions of the course (high frequency of use tools will have a large impact on success)</li>
	<li>The central tendency will prevail so that models that aggregate variables about student interaction may lead to over/under estimation</li>
</ol>
<h3>Method</h3>
Correlational (non-experimental) design. 9 first year courses that were part of an institutional project on retention. Participation in that project based on a discipline specific low level of retention - a quite low 20% (at least to me).  4134 students from 9 courses over 5 years - not big numbers.

Outcome variables - <em>percent mark</em> and <em>academic status</em> - pass, fail, or withdrawn (n=88).

<strong>Data </strong>based on other studies and availability
<ul>
	<li>Student characteristics: age, gender, international student, language at home, home remoteness, term access, previous enrolment, course start.</li>
	<li>LMS trace data: usage of various tools, some continuous, some lesser used as dichotomous and then categorical variables (reasons given)</li>
</ul>
Various statistics tests and models used.

Discussion

Usage across courses was variable hence the advice (p. 79)
<ol>
	<li>there is a need to create models for academic success prediction for individual courses, incorporating instructional conditions into the analysis model.</li>
	<li>there must be careful consideration in any interpretation of any predictive model of academic success, if these models do not incorporate instructional conditions</li>
	<li>particular courses,which may have similar technology use,maywarrant separatemodels for academic suc- cess prediction due to the individual differences in the enrolled student cohort.</li>
</ol>
And
<p style="padding-left: 30px;">we draw two important conclusions: a) generalized models of academic success prediction can overestimate or underestimate effects of individual predictors derived from trace data; and b) use of a specific LMS feature by the students within a course does not necessarily mean that the feature would have a significant effect on the students' academic success; rather, instructional conditions need to be considered in order to understand if, and why, some variables were significant in order to inform the research and practice of learning and teaching (pp. 79, 81)</p>
Closes out with some good comments on moving students/teachers beyond passive consumers of these models and the danger of existing institutional practice around analytics having decisions be made too far removed from the teaching context.

&nbsp;</p>

</body>
</html>
