<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<title>Learning analytics, intervention and helping teachers</title>
</head>
<body>
<h1><a href="https://djon.es/blog/2013/06/07/learning-analytics-intervention-and-helping-teachers/">Learning analytics, intervention and helping teachers</a></h1>

<p>Tags: elearning, indicators</p>

<p>It seems to be the day for a backlash against learning analytics or its parent big data. This morning my PLN has filtered to the top <a href="http://www.wired.com/opinion/2013/02/big-data-means-big-errors-people/">Taleb's "Beware the big errors of 'big data'"</a> and <a href="http://mobile.nytimes.com/blogs/bits/2013/06/01/why-big-data-is-not-truth/">Why big data is not truth</a>. Not that surprising to me given that <a href="https://djon.es/blog/2012/11/01/moving-beyond-a-fashion-likely-paths-and-pitfalls-for-learning-analytics/">I've argued</a> that learning analytics in Universities has all hallmarks of yet another fad.

As I mentioned <a href="https://djon.es/blog/2012/11/01/moving-beyond-a-fashion-likely-paths-and-pitfalls-for-learning-analytics/">in this presentation</a> my argument isn't that "Learning analytics == crap". My argument is that "How universities (and most organisations) for that matter implement learning analytics == crap".  After all - to paraphrase some Sir Ken Robinson schtick about lifting standards - no-one is arguing against data-driven decision making (which is a main claim of big data). All decisions should be based on data. The problem is the reality of most implementations will be that the data provided by learning analytics is likely to be horribly flawed, provided to the wrong people, and used to make decisions very badly.

As it happens, <a href="http://damosworld.wordpress.com/">Damien</a>, <a href="http://beerc.wordpress.com/">Col</a> and I are all working on various related projects that are trying to figure out how to implement learning analytics within universities in a way that doesn't equate to animal waste product.  The following summarises some thinking out loud and the following of some initial ideas and will hopefully inform what I'll be <a href="https://djon.es/blog/2013/05/12/moodle-bim-reflective-journals-and-tpack-suggestions-for-moving-beyond/">talking about</a> at Moodlemoot'AU 2013 in a couple of weeks.

In the end, this is more just an early exploration of Performance Support Systems literature that has shown some level of support for a direction we're exploring, i.e. embedding learning analytics into the LMS and other tools currently being used to better enable action.

<h3>Performance support systems</h3>

A common refrain heard when institutional folk get together and chat about learning analytics goes something like this<blockquote>But we already have all this data, why don't we use it? <br />Because it's all in separate systems.<br />So we'll give lots of money to Vendor X to implement another piece of technology that will bring all this data together and provide dashboards for people to look at the data.</blockquote>

The only winner out of this approach is the vendor who chalks up another sale. The data could have been brought together via any number of technical means, probably at much less cost, and in a way that doesn't tie the organisation to accessing the data through the reporting tool provided by the vendor. Dashboards are generally a waste of time because people don't use them. Especially teaching staff. Even if the dashboards can provide the sort of contextual information that will help a teacher intervene with a student, the information is provided in a system that is a million miles away from the system where the teacher will intervene.

I've long through that the obvious lens for looking at this is the long quoted idea of "Performance Support".  The following is an initial exploration of some of the literature.

Raybould (1995, p. 11) offers the following definition with some added emphasis from me<blockquote><ul>
  <li> Encompasses <strong>all the software</strong> needed to <strong>support the work of individuals</strong> (not just one or two specific software applications).</li>
  <li> <strong>Integrates knowledge assets into the interface of the software tools</strong>, rather than separating them as add-on components. For example, company policy information may be presented in a dialog box message rather than in a separate online document.</li>
  <li> Looks at the <strong>complete cycle</strong> including the capture process as well as the distribution process.</li>
  <li> Includes the management of nonelectronic as well as electronic assets.</li>
</ul></blockquote>

While not without flaw, the definition does get at some of what I'm interested in, including
<ul>
  <li>
  <li> Integrating all of the knowledge required to complete a task into the tool I use to complete the task. </li>
  <li> Consider the complete cycle.<p>i.e. just don't build the data warehouse and expect it to be used. Think about how people can be supported in using the data. After all the EDUCAUSE definition of learning analytics is (my emphasis added)<blockquote>the use of data, statistical analysis, and explanatory and predictive models to gain insights and <strong>act</strong> on complex issues.</blockquote> </li>
  <li> All brought together with a focus on the "work of individuals".<p>i.e. don't provide generic dashboards and leave it to the teachers to figure out what to do. Figure out what tools can be built to help teachers perform their work better.</p></li>
</ul>

<h3>Can't see the trees for the forest</h3>

Which brings up the biggest barrier. The processes, systems, structures and people set up to implement "enterprise" systems like learning analytics focus on the forest. Or if they have any conception of the trees in the forest, the trees are all pine trees of identical size, shape and requirements. Being able to see the trees, to focus on the work of the individuals, is not easy. But that's not the point. The point is that enterprise systems and processes can never effectively focus on the "work of individuals". They don't even try.

And it's important, at least for this argument. Villachica et al (2006, p. 540) argue that the purpose of PSS is "expert-like performance from day 1 with little or no training" and that this can only occur within an appropriate zone - the "performance zone".

<a href="http://www.flickr.com/photos/david_jones/8974958662/" title="The Performance Zone by David T Jones, on Flickr"><img alt="The Performance Zone" height="366" src="http://farm8.staticflickr.com/7356/8974958662_0a08817144.jpg" width="414" /></a>

Oh and here's a good quote that reinforces my point above<blockquote>There is also widespread
agreement that maintaining performance within this zone requires users to be able to learn, use, and reference necessary information within a single context and without breaks in the natural flow of performing their jobs. (Villachica et al, 2006, p. 540)</blockquote> I can see myself using that a few times.

Kert and Kurt (2012, p. 486) cite Sleight (1998) to identify the following requirements (amongst others) of an EPSS
<ul>
  <li> computer assisted. </li>
  <li> accessible exactly at the time the task is realised. </li>
  <li> in the study environment. </li>
  <li> controllable by the user </li>
  <li> ability to easily bring it up to date and fast access to information. </li>
</ul>


<h3>References</h3>

Kert, S. B., &amp; Kurt, A. A. (2012). The effect of electronic performance support systems on self-regulated learning skills. Interactive Learning Environments, 20(6), 485–500.

Raybould, B. (1995). Performance Support Engineering : An Emerging Development Methodology for Enabling Organizational Learning. Performance improvement quarterly, 8(1), 7–22.

Villachica, S., Stone, D., &amp; Endicott, J. (2006). Performance Suport Systems. In J. Pershing (Ed.), Handbook of Human Performance Technology (Third Edit., pp. 539–566). San Francisco, CA: John Wiley &amp; Sons.</p>

</body>
</html>
