<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<title>Strategies for curriculum mapping and data collection for assuring learning</title>
</head>
<body>
<h1><a href="https://djon.es/blog/2013/10/10/strategies-for-curriculum-mapping-and-data-collection-for-assuring-learning/">Strategies for curriculum mapping and data collection for assuring learning</a></h1>

<p>Tags: curriculumMapping, elearning</p>

<p>The following is a summary of and some reaction to the final report of an <a href="http://www.olt.gov.au/project-hunters-and-gatherers-strategies-curriculum-mapping-and-data-collection-assuring-learning-20">OLT funded project</a> titled "Hunters and gatherers: strategies for curriculum mapping and data collection for assuring learning".  This appears to be the <a href="http://assuringlearning.com/">project website</a>.

My interest arises from the "Outcomes and Analytics Project" that's on my list of tasks. That project is taking a look at Moodle's new outcomes support (and perhaps other tools) that can be leveraged by a Bachelor of Education and try to figure out what might be required to gain some  benefit from those tools (and whether it's worth it).

<h2>Reactions</h2>

The recommended strategies (holistic, integrated, collaborative, maintainable) could form a good set of principles for some of what I'm thinking.

In terms of gathering data on student performance, assessment and rubrics appear to be the main method.  Wonder if analytics and other approaches can supplement this?

It would appear that no-one is doing this stuff very well.  The best curriculum mapping tool is a spreadsheet!!!  And the data gathering tools are essentially assignment marking tools.  Neither set of tools were well evaluated in terms of ease of use.

Lots of good principles and guidelines for implementation, but crappy tools.

<h3>Student centered much?</h3>

AoL is defined as determining program learning outcomes and standards and then gathering evidence to measure student performance. use for curriculum development, continuous improvement and accrediatation - but no mention is made of helping students develop eportfolios for employers.  Especially in professional programs with national standards, this would seem an obvious overlap.  Student centered much?

Interesting given that AoL is meant to be based on student centered learning.

<h3>Staff engagement</h3>

Is positioned as a difficulty.

<h3>Facilitating</h3>

AN earlier project (on which this was built) was titled "Facilitating staff and student engagement with graduate attribute development". Apparently limited to helping staff design criteria to assess GAs and students self-evaluate against those.  Seems to be only a small part of the facilitation process.

<h3>GAs versus professional standards</h3>

I have to admit to being deeply skeptical around the notation of institutional graduate attributes.  Always struck me as a myth created by high-priced senior management to justify what was unique about the vision they were creating and hence with little connection to the reality of teaching at a university. In particular, that the standards/attributes set by the professional bodies associated with certain disciplines would always count for more.

Of course institutional GAs have been apparently required by the government since 1992. I wonder if this is like some of those other legal requirements that have been discovered to have never existed, ceased to exist or which were misinterpreted?

<h2>Executive summary</h2>

Assurance of Learning (AoL) "evaluates how well an institution accomplishes the educational aims at the core of its activities"...AoL provides "qualitative and quantitative indicators for the assessment of the quality of award courses"  Thus be used for
<ul>
  <li> institutional/management ends: strategic directions, priorities, quality assurance, enhancement processes. </li>
  <li> individual curriculum development. </li>
  <li> Valid evidence to external constitutents. </li>
</ul>

Focus of this project on
<ol>
  <li> <strong>mapping</strong> program learning outcomes. </li>
  <li> <strong>Collecting data</strong> on student performance against each learning objective. </li>
</ol>

<em>Aside:</em> interesting that they've already used both outcome and objective. Does this mean these are different concepts, or just different labels for the same concept?

Investigation was done via
<ul>
  <li> Exploratory interviews with 25 of 39 associate deans L&amp;T from Oz Unis. </li>
  <li> 8 focus groups with 4 good practice institutions, 2 at each institution - one with a senior leader, the other with teaching staff. </li>
  <li> Delphi method - but who with? </li>
  <li> Interviews with experts. </li>
  <li> online survey. </li>
</ul>

Recommended good practice strategies
<ul>
  <li> <strong>holistic</strong> - whole of program, to ensure student's progress and introduction of GAs prior to demonstration. </li>
  <li> <strong>integrated</strong> - GAs must be embedded in the curriculum and links to assessment. </li>
  <li> <strong>collaborative</strong> - developed with teaching in an inclusive - not top down - approach to engage staff. </li>
  <li> <strong>maintainable</strong> - must not be reliant on particular individuals or extra-ordinary resources. </li>
</ul>

They then proceed to mention typical cultural change strategies.

Also did an <a href="http://assuringlearning.com/tools.html">independent review of existing tools</a>.  Interestingly the Blackboard 9.1 goals/standards service gets a mention.

<h2>Chapter 1 - Project overview</h2>

Identifies 7 key stages in assuring learning from an AACSB White Paper
<ol>
  <li> establishing graduate attributes and measurable learning outcomes for the program;</li>
  <li>  mapping learning outcomes to suitable units of study in the program (where possible allowing for introduction, further development and then assurance of the outcomes);</li>
  <li>  aligning relevant assessment tasks to assure learning outcomes;</li>
  <li> communicating learning outcomes to students;</li>
  <li>  collecting data to show student performance for each learning objective;</li>
  <li>  reporting student performance in the learning outcomes;</li>
  <li>  reviewing reports to identify areas for program development (‘Closing the Loop’).</li>
</ol>

Explains the growing requirement for this, lots of acronyms and literature.

Mentions their prior project which development the ReView online assessment system to help staff develop criteria that assessed GAs within the set assignments.  Students have self-evaluations.

Project aims to inform strategy to identify efficient and manageable assurance mechanisms (<em>effective not important?</em>).

<h2>Chapter 2 - methodology</h2>

Key guiding questions
<ol>
  <li> How is mapping of GAs being done? </li>
  <li> How is the collection of GA data being done? </li>
  <li> What are the main challenges in mapping and collecting? </li>
  <li> Are there identifiable good practice principels? </li>
  <li> What are the tools currently being used? </li>
</ol>

<h2>Chapter 3 - Literature review</h3>

<h3>here come the standards</h3>

<blockquote>Standards are defined as “the explicit levels of attainment required of and achieved by students and graduates, individually and collectively, in defined areas of knowledge and skills” (TEQSA, 2011, p. 3)......Academic standards are learning outcomes described in terms of core discipline knowledge and core discipline-specific skills, and expressed as the minimum learning outcomes that a graduate of any given discipline (or program) must have achieved (Ewan, 2010).</blockquote>

TEQSA is apparently requiring academic standards "be expressed as measurable or assessable learning outcomes".

Determining standards and then collecting data against those is complex. Coates (2010) acknowledges the complexity and suggests a need for cultural change. And there is apparently an urgent need for "new, efficient and effective ways of judging and warranting" (Oliver, 2011, p. 3).

<h3>Extant literature</h3>

AoL finds its pedagogical basis in student-centered learning.

Curriculum mapping in AOL is the process of embedding learning outcomes related to GAS into units of study where these are introduced, developed and then assured.

AUQA required curriculum mapping as do most professional accrediting bodies - hence the 2009 observation from Barrie (et al 2009) that most Australian universities have some sort of strategic project underway.

The higher ed mapping literature is scant but suggests it's useful for (all backed up with citations)
<ul>
  <li> identifying gaps in a program </li>
  <li> monitoring course diversity and overlap</li>
  <li> providing opportunity for reflection and discourse </li>
  <li> reducing confusion and overlap and increasing coherence</li>
</ul>

There are more, but there does seem to be some overlap.

Mentions the problem of the compliance culture, others include
<ul>
  <li> difference between the intended and the enacted curriculum from the students' perspective </li>
  <li> how to contextualise GAs into a discipline. </li>
  <li> mapping seen as threatening, as a course cutting exercise, criticisms of teaching material etc. </li>
  <li> labour intensive exericse. </li>
</ul>

Staff engagement is seen as the key and current suggestions for improvement include
<ul>
  <li> develop a conceptual framework for developing GAs, including 3 elements
         <ol>
            <li> clear statement of purpose for curriculum mapping. </li>
             <li> a tool that allows an aggregate view of a course. </li>
             <li> a process for use of the tool </li>
           </ol>
  </li>
  <li> map GAs using extensive audits of each course. </li>
  <li> a cyclical process including visual representations to enable a fluid/adaptable curriculum </li>
  <li> availability of sufficient resources. </li>
  <li> use of alignment templates (isn't this a tool?) </li>
  <li> professional development to integrate and contextualise GAs. </li>
  <li> having specialists who can teach a particular attribute. </li>
  <li> whole of program approach, focus on team co-operation and more time spent on design. </li>
  <li> staff support where workloads increase. </li>
  <li> linkages between GA development and professional development. </li>
</ul>


<h3>Embedding versus standardised testing</h3>

Mention of various standardised tests at the end of study. Talks about plusses and minuses.

<h3>Data collection for AoL</h3>

Focused on entering student performance outcomes against each learning objective.  Need a "systematic method to collect data and explore the achievement levels of students in each of the selected attributes" to inform on-going development.

There are challenges in collecting and providing evidence - highlighting the need for efficiency and streamlining the process.

Assessment rubrics (formative and summative) are key. But there are challenges.  Don't want a "tick list". Some skills are ill-defined, overlapping and difficult to measure. And the question of standardisation - homegenisation or pursuit of common goals. Multiple interpretations of criteria.

Rubrics can become to be used for comparison between institutions; assurance of content/process/outcomes across courses.

<h3>Continuous improvement/closing the loop</h3>

Apparently the "raison d'etre for assessing student learning" and also something that institutions are "most confused about how to go about closing the loop (Martell, 2007)"....."integration of the assessment of learning outcomes into developmental approaches in the classroom has been somewhat intangible (Tayloer et al, 2009)".

<h2>Curriculum mapping</h2>

important features for selecting a CM system to support AoL
<ul>
  <li> support an inclusive and participatory process; </li>
  <li> foster a program-wide approach to produce a mapped overview; </li>
  <li> map by assessment task; </li>
  <li> develop student awareness of attributes and their distribution within the program; </li>
</ul>

The standout tool was a spreadsheet!

<h2>Data collection</h2>

Important features for a data collection system included
<ul>
  <li> implement a consistent criteria for attributes across programs; </li>
  <li> extract outcome-specific data; </li>
  <li> embed measurement in the curriculum </li>
  <li> produce built-in reports; </li>
  <li> conduct analysis for closing the loop; </li>
  <li> implement multiple measures of AoL for program wide view. </li>
</ul>

<a href="http://www.review-edu.com/">ReView</a> was seen as the stand out.  But then, it arose from the last OLT project. But then that makes this comment interesting<blockquote>ReView does not rate that well on ‘ease of use without the need for much supplementary professional development</blockquote></p>

</body>
</html>
