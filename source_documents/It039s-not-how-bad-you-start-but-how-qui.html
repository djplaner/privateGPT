<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<title>It&#039;s not how bad you start, but how quickly you get better</title>
</head>
<body>
<h1><a href="https://djon.es/blog/2015/09/03/its-not-how-bad-you-start-but-how-quickly-you-get-better/">It&#039;s not how bad you start, but how quickly you get better</a></h1>

<p>Tags: 4paths, bad</p>

<p>Wood &amp; Hollnagel (2006) start by presenting the Bounded Rationality syllogism<blockquote> All cognitive systems are finite (people, machines, or combinations).
All finite cognitive systems in uncertain changing situations are fallible.
Therefore, machine cognitive systems (and joint systems across people and machines) are fallible. (p. 2)</blockquote>

From this they suggest that<blockquote>The question, then, is not fallibility or finite resources of systems, but rather the development of strategies that handle the fundamental tradeoffs produced by the need to act in a finite, dynamic, conflicted, and uncertain world.

The core ideas of Cognitive Systems Engineering (CSE) shift the question from
overcoming limits to supporting adaptability and control
(p. 2)</blockquote>

Which has obvious links to my last post, <a href="https://davidtjones.wordpress.com/2015/08/28/all-models-are-wrong-but-some-are-useful-and-its-application-to-e-learning/">"All models are wrong"</a>.

This is why organisations annoy me with their fetish for developing the one correct model (or system) and requiring that everyone should and can follow that one correct model.</p>

</body>
</html>
