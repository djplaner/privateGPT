<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<title>Choosing your indicators - why, how and what</title>
</head>
<body>
<h1><a href="https://djon.es/blog/2008/08/31/choosing-your-indicators-why-how-and-what/">Choosing your indicators - why, how and what</a></h1>

<p>Tags: blackboardIndicators, c2d2, complexityLeadership, courseSites, elearning, Uncategorized</p>

<p>The <a href="http://cddu.cqu.edu.au/">unit</a> I work with is undertaking a project called <a href="http://cddu.cqu.edu.au/index.php/Blackboard_Indicators">Blackboard Indicators</a>. Essentially the development of a tool that will perform some automated checks on our <a href="http://www.cquni.edu.au/">institution's</a> Blackboard course sites and show some indicators which might identify potential problems or areas for improvement.

The current status is that we're starting to develop a slightly better idea of what people are currently doing through use of the literature and also some professional networks (e.g. the <a href="http://www.acode.edu.au/">Australasian Council on Open, Distance and E-learning</a>) and have an initial prototype running.

Our current problem is how do you choose what the indicators should be?  What are the types of problems you might see? What is a "good" course website?

<h3>Where are we up to?</h3>

Our initial <a href="http://cddu.cqu.edu.au/index.php/Category:Blackboard_Indicators">development work</a> has focused on three groupings of category: course content, coordinator presence and all interactions.  Some more detail on this <a href="http://cq-pan.cqu.edu.au/david-jones/blog/?p=204">previous post</a>.

<a href="http://cddu.cqu.edu.au/index.php/Colin_Beer">Colin Beer</a> has contributed some additional thinking about some potential indicators in a <a href="http://beerc.wordpress.com/2008/08/30/lms-inidcators-project/">recent post</a> on his blog.

Col and I have talked about using our blogs and other locations to talk through what we're thinking to develop a concrete record of our thoughts and hopefully generate some interest from other folk.

Col's list includes
<ul>
  <li> Learner. </li>
  <li>  Instructor. </li>
  <li> Content. </li>
  <li> Interactions: learner/learner, learner/instructor, learner/content, instructor/content </li>
</ul>

<h3>Why and what?</h3>

In identifying a list of indicators, as when trying to evaluate anything, it's probably a good idea to start with a clear definition of why you are starting on this, what are you trying to achieve.

The stated purpose of this project is to help us develop a better understanding of how and how well staff are using the Blackboard courses sites. In particular, we want to know about any potential problems (e.g. a course site not being available to students) that might cause a large amount of "helpdesk activity". We would also like to know about trends across the board which might indicate the need for some staff development, improvements in the tools or some support resources to improve the experience of both staff and students.

There are many other aims which might apply, but this is the one I feel most comfortable with, at the moment.

Some of the other aims include
<ul>
  <li> Providing academic staff with a tool that can aid them during course site creation by checking their work and offering guidance on what might be missing. </li>
  <li> Provide management with a tool to "check on" course sites they are responsible for. </li>
  <li> Identify correlations between characteristics of a course website and success. </li>
</ul>

The constraints we need to work within include
<ul>
  <li> Little or no resources - implication being that manual, human checking of course sites is not currently a possibility. </li>
  <li> Difficult organisational context due to on-going restructure - which makes it hard to get engagement from staff in a task that is seen as additional to existing practice and also suggests a need to be helping staff deal with existing problems more so than creating more work. A need to be seen to be working with staff to improve and change, rather than being seen as inflicting change upon them.
  </li>
  <li> LMS will be changing - come 2010 we'll be using a new LMS, whatever we're doing has to be transportable. </li>
</ul>

<h3>How?</h3>

From one perspective there are two types of process which can be used in a project like this
<ol>
  <li> Teleological or idealist.<br />
       A group of experts get together, decide and design what is going to happen and then explain to everyone else why they should use it and seek to maintain obedience to that original design.
  </li>
  <li> Ateleological or naturalist.<br />
      A group of folk, including significant numbers of folk doing real work, collaborate together to look at the current state of the local context and undertake a lot of small scale experiments to figure out if anything makes sense, they examine and reflect on those small scale experiments and chuck out the ones that didn't work and build on the ones that did.
  </li>
</ol>

(For more on this check out: this <a href="http://video.google.com/videoplay?docid=-5567968733907010214&amp;hl=en">presentation video</a> or this <a href="http://video.google.com/videoplay?docid=3241775154462848671&amp;hl=en">presentation video</a> or this <a href="http://www.ascilite.org.au/conferences/singapore07/procs/jones-d.pdf">paper</a> or this <a href="http://cq-pan.cqu.edu.au/david-jones/Publications/Papers_and_Books/">one</a>.)

From the biased way I explained the choices I think it's fairly obvious which approach I prefer.  A preference for the atelelogical approach also means that I'm not likely to want to spend vast amounts of time evaluating and designing criteria based on my perspectives.  It's more important to get a set of useful indicators up and going, in a form that can be accessed by folk and have a range of processes by which discussion and debate is encouraged and then fed back into the improvement of the design.

The on-going discussion about the project is more likely to generate something more useful and contextually important than large up-front analysis.

<h3>What next then?</h3>

As a first step, we have to get something useful (for both us and others) up and going in a form that is usable and meaningful.  We then have to engage with them and find out what they think and where they'd like to take it next.  In parallel with this is the idea of finding out, in more detail, what other institutions are doing and see what we can learn.

The engagement is likely going to need to be aimed at a number of different communities including
<ul>
  <li> Quality assurance folk: most Australian universities have quality assurance folk charged with helping the university be seen by <a href="http://www.auqa.edu.au/">AUQA</a> as being good.<br />
       This will almost certainly, eventually, require identifying what are effective/good outcomes for a course website as outcomes are a main aim for the next AUQA round.
  </li>
  <li> Management folk: the managers/supervisors at CQU who are responsible for the quality of learning and teaching at CQU.</li>
  <li> Teaching staff: the people responsible for creating these artifacts.</li>
  <li> Students: for their insights. </li>
</ul>

Initially, the indicators we develop should match our stated aim - to identify problems with course sites and become more aware with how they are being used. To a large extent this means not worrying about potential indicators of good outcomes and whether or not there is a causal link.

I think we'll start discussing/describing the indicators we're using and thinking about on a <a href="http://cddu.cqu.edu.au/index.php/Types_of_Blackboard_Indicators">project page</a> and we'll see where we go from there.</p>

</body>
</html>
